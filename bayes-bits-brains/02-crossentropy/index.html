<!DOCTYPE html><html lang="en" class="__variable_871a3d __variable_e566b6"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/bayes-bits-brains/_next/static/media/1f3fe8c6df3d47c1-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/bayes-bits-brains/_next/static/media/558ca1a6aa3cb55e-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="/problens-web/02-crossentropy/crossentropy_entropy_formula.png"/><link rel="stylesheet" href="/bayes-bits-brains/_next/static/css/dbd2812aeb31ae93.css" data-precedence="next"/><link rel="stylesheet" href="/bayes-bits-brains/_next/static/css/5eacd01f773eed7f.css" data-precedence="next"/><link rel="stylesheet" href="/bayes-bits-brains/_next/static/css/622677e989ec6b9c.css" data-precedence="next"/><link rel="stylesheet" href="/bayes-bits-brains/_next/static/css/8870e1553eaf2585.css" data-precedence="next"/><link rel="stylesheet" href="/bayes-bits-brains/_next/static/css/13ab9833c5f6a889.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/bayes-bits-brains/_next/static/chunks/webpack-9a476a05d384b821.js"/><script src="/bayes-bits-brains/_next/static/chunks/11eacf67-36e122528944b7c0.js" async=""></script><script src="/bayes-bits-brains/_next/static/chunks/954-75be81b03e98094e.js" async=""></script><script src="/bayes-bits-brains/_next/static/chunks/main-app-68bf59435228f0c4.js" async=""></script><script src="/bayes-bits-brains/_next/static/chunks/45233be0-cba42a3c5454822d.js" async=""></script><script src="/bayes-bits-brains/_next/static/chunks/4bbad5ef-2b7eebeba98386a5.js" async=""></script><script src="/bayes-bits-brains/_next/static/chunks/f885ef2c-649d91f122a11dff.js" async=""></script><script src="/bayes-bits-brains/_next/static/chunks/0133708f-65d46efc97e0f93d.js" async=""></script><script src="/bayes-bits-brains/_next/static/chunks/91c6c604-01c8ba6fb8219fe8.js" async=""></script><script src="/bayes-bits-brains/_next/static/chunks/950-e6e0e01ad2983f53.js" async=""></script><script src="/bayes-bits-brains/_next/static/chunks/390-7b8633ed8924eb08.js" async=""></script><script src="/bayes-bits-brains/_next/static/chunks/582-781968895face3a7.js" async=""></script><script src="/bayes-bits-brains/_next/static/chunks/app/%5B...path%5D/page-4c83be7a70b80924.js" async=""></script><meta name="next-size-adjust" content=""/><title>Bayes, bits &amp; brains</title><meta name="description" content="Bayes, bits &amp; brains"/><script src="/bayes-bits-brains/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="antialiased"><div class="min-h-screen flex flex-col bg-white"><header class="sticky left-0 right-0 top-0 z-50 bg-white/80 backdrop-blur-sm gap-0 transition-all duration-200 py-2"><div class="max-w-[var(--content-width)] mx-auto px-4 py-6 flex h-16 items-center justify-end"><button class="lg:hidden p-2 text-neutral-600 hover:text-neutral-900" aria-label="Toggle menu"><svg class="h-6 w-6" fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" stroke="currentColor"><path d="M4 6h16M4 12h16M4 18h16"></path></svg></button></div></header><div class="fixed inset-0 z-40 lg:hidden transition-opacity duration-300 opacity-0 pointer-events-none"><div class="top-16 fixed inset-0 bg-neutral-100/80 backdrop-blur-sm shadow-xl"></div><div class="top-16 fixed inset-x-0 bottom-0 bg-white shadow-xl transition-transform duration-300 overflow-y-auto -translate-y-full"><div class="w-full min-h-full" style="margin-left:calc(max((100% - var(--content-width)) / 2 + 1rem, 2rem))"><nav class="Sidebar_sidebar__G7Mzs py-6 text-lg"><div class="mb-6"><a class="text-2xl font-semibold text-neutral-900 hover:text-neutral-600 transition-colors" href="/bayes-bits-brains/">Bayes, bits &amp; brains</a></div><div class="Sidebar_scrollableContent__WtZmB"><div><ul class="Sidebar_list__I5HfV"><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/00-riddles/">Riddles</a><div class="Sidebar_subsections__4JMe7 false"><div class="Sidebar_subsectionWrapper__tUx1X Sidebar_subsectionWrapperHidden__3D_C_"></div></div></li></ul></div><div><div class="my-4 border-t border-gray-200"></div><ul class="Sidebar_list__I5HfV"><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/01-kl_intro/">Bayes &amp; KL divergence</a><div class="Sidebar_subsections__4JMe7 false"><div class="Sidebar_subsectionWrapper__tUx1X Sidebar_subsectionWrapperHidden__3D_C_"></div></div></li><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/02-crossentropy/">Crossentropy &amp; Entropy</a><div class="Sidebar_subsections__4JMe7 false"><div class="Sidebar_subsectionWrapper__tUx1X Sidebar_subsectionWrapperHidden__3D_C_"></div></div></li><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/03-entropy_properties/">KL &amp; Entropy properties</a><div class="Sidebar_subsections__4JMe7 false"><div class="Sidebar_subsectionWrapper__tUx1X Sidebar_subsectionWrapperHidden__3D_C_"></div></div></li></ul></div><div><div class="my-4 border-t border-gray-200"></div><ul class="Sidebar_list__I5HfV"><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/04-mle/">Maximum likelihood</a><div class="Sidebar_subsections__4JMe7 false"><div class="Sidebar_subsectionWrapper__tUx1X Sidebar_subsectionWrapperHidden__3D_C_"></div></div></li><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/05-max_entropy/">Maximum entropy</a><div class="Sidebar_subsections__4JMe7 false"><div class="Sidebar_subsectionWrapper__tUx1X Sidebar_subsectionWrapperHidden__3D_C_"></div></div></li><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/07-machine_learning/">Loss functions</a><div class="Sidebar_subsections__4JMe7 false"><div class="Sidebar_subsectionWrapper__tUx1X Sidebar_subsectionWrapperHidden__3D_C_"></div></div></li></ul></div><div><div class="my-4 border-t border-gray-200"></div><ul class="Sidebar_list__I5HfV"><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/08-coding_theory/">Coding theory</a><div class="Sidebar_subsections__4JMe7 false"><div class="Sidebar_subsectionWrapper__tUx1X Sidebar_subsectionWrapperHidden__3D_C_"></div></div></li><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/09-kolmogorov/">Kolmogorov complexity</a><div class="Sidebar_subsections__4JMe7 false"><div class="Sidebar_subsectionWrapper__tUx1X Sidebar_subsectionWrapperHidden__3D_C_"></div></div></li></ul></div><div class="mt-6 pt-4 border-t border-gray-200"><ul class="Sidebar_list__I5HfV"><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/about/">About</a></li><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/resources/">Resources</a></li><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/bonus/">Graveyard</a></li></ul></div></div></nav></div></div></div><div class="flex-1"><nav class="Sidebar_sidebar__G7Mzs xl:w-60 lg:w-52 hidden lg:block fixed top-16 bottom-0 pt-16 text-base" style="left:var(--sidebar-offset)"><div class="mb-6"><a class="text-2xl font-semibold text-neutral-900 hover:text-neutral-600 transition-colors" href="/bayes-bits-brains/">Bayes, bits &amp; brains</a></div><div class="Sidebar_scrollableContent__WtZmB"><div><ul class="Sidebar_list__I5HfV"><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/00-riddles/">Riddles</a><div class="Sidebar_subsections__4JMe7 false"><div class="Sidebar_subsectionWrapper__tUx1X Sidebar_subsectionWrapperHidden__3D_C_"></div></div></li></ul></div><div><div class="my-4 border-t border-gray-200"></div><ul class="Sidebar_list__I5HfV"><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/01-kl_intro/">Bayes &amp; KL divergence</a><div class="Sidebar_subsections__4JMe7 false"><div class="Sidebar_subsectionWrapper__tUx1X Sidebar_subsectionWrapperHidden__3D_C_"></div></div></li><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/02-crossentropy/">Crossentropy &amp; Entropy</a><div class="Sidebar_subsections__4JMe7 false"><div class="Sidebar_subsectionWrapper__tUx1X Sidebar_subsectionWrapperHidden__3D_C_"></div></div></li><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/03-entropy_properties/">KL &amp; Entropy properties</a><div class="Sidebar_subsections__4JMe7 false"><div class="Sidebar_subsectionWrapper__tUx1X Sidebar_subsectionWrapperHidden__3D_C_"></div></div></li></ul></div><div><div class="my-4 border-t border-gray-200"></div><ul class="Sidebar_list__I5HfV"><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/04-mle/">Maximum likelihood</a><div class="Sidebar_subsections__4JMe7 false"><div class="Sidebar_subsectionWrapper__tUx1X Sidebar_subsectionWrapperHidden__3D_C_"></div></div></li><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/05-max_entropy/">Maximum entropy</a><div class="Sidebar_subsections__4JMe7 false"><div class="Sidebar_subsectionWrapper__tUx1X Sidebar_subsectionWrapperHidden__3D_C_"></div></div></li><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/07-machine_learning/">Loss functions</a><div class="Sidebar_subsections__4JMe7 false"><div class="Sidebar_subsectionWrapper__tUx1X Sidebar_subsectionWrapperHidden__3D_C_"></div></div></li></ul></div><div><div class="my-4 border-t border-gray-200"></div><ul class="Sidebar_list__I5HfV"><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/08-coding_theory/">Coding theory</a><div class="Sidebar_subsections__4JMe7 false"><div class="Sidebar_subsectionWrapper__tUx1X Sidebar_subsectionWrapperHidden__3D_C_"></div></div></li><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/09-kolmogorov/">Kolmogorov complexity</a><div class="Sidebar_subsections__4JMe7 false"><div class="Sidebar_subsectionWrapper__tUx1X Sidebar_subsectionWrapperHidden__3D_C_"></div></div></li></ul></div><div class="mt-6 pt-4 border-t border-gray-200"><ul class="Sidebar_list__I5HfV"><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/about/">About</a></li><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/resources/">Resources</a></li><li class="Sidebar_item__IbyhT"><a class="Sidebar_link__NjnkI " href="/bayes-bits-brains/bonus/">Graveyard</a></li></ul></div></div></nav><main class="Page_main__cWiGN max-w-[var(--content-width)] mx-auto px-4 pt-12 pb-8"><article class="Page_article__A_Nud prose prose-neutral max-w-none"><h1>KL properties &amp; (cross-)entropy</h1>
<p>In this chapter, we&#x27;ll see how KL divergence can be split into two pieces called <em>cross-entropy</em> and <em>entropy</em>.</p>
<div class="my-6 rounded-lg overflow-hidden bg-amber-50 border border-amber-200"><div class="px-4 py-3 border-b border-amber-200"><div class="font-medium text-base text-amber-800 flex items-center gap-2"><span class="text-lg">💡</span>If there is one thing you remember from this chapter...</div></div><div class="px-4 py-3 text-base leading-relaxed text-gray-700"><p><img src="/problens-web/02-crossentropy/crossentropy_entropy_formula.png" alt="Formula for KL divergence" style="cursor:pointer" class=""/>
Cross-entropy measures the average surprisal of model <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span></span></span></span> on data from <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span></span></span></span>. When <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mi>q</mi></mrow><annotation encoding="application/x-tex">p = q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span></span></span></span>, we call it entropy.</p></div></div>
<h2>KL = cross-entropy - entropy</h2>
<p>Quick refresher: In the <a href="/01-kl_intro">previous chapter</a>, we saw how KL divergence comes from repeatedly using Bayes&#x27; theorem with log-space updating:</p>
<div class="p-6 bg-gray-50 rounded-lg space-y-4 max-w-4xl mx-auto"><h3 class="text-lg font-semibold text-center text-gray-800">Bayes Sequence Explorer (Log Space)</h3><div class="bg-white rounded-lg p-6 space-y-4"><div class="flex items-center justify-center space-x-4 mb-6"><span class="text-sm font-medium text-gray-700">Coin sequence:</span><div class="flex space-x-1"><div class="w-8 h-8 flex items-center justify-center rounded border-2 font-mono font-bold cursor-pointer bg-gray-100 border-gray-300 text-gray-500">H</div><div class="w-8 h-8 flex items-center justify-center rounded border-2 font-mono font-bold cursor-pointer bg-gray-100 border-gray-300 text-gray-500">T</div><div class="w-8 h-8 flex items-center justify-center rounded border-2 font-mono font-bold cursor-pointer bg-gray-100 border-gray-300 text-gray-500">T</div><div class="w-8 h-8 flex items-center justify-center rounded border-2 font-mono font-bold cursor-pointer bg-gray-100 border-gray-300 text-gray-500">H</div><div class="w-8 h-8 flex items-center justify-center rounded border-2 font-mono font-bold cursor-pointer bg-gray-100 border-gray-300 text-gray-500">T</div></div><button class="px-3 py-1 bg-gray-200 text-gray-700 rounded text-sm hover:bg-gray-300">Edit</button><button class="px-3 py-1 bg-blue-500 text-white rounded text-sm hover:bg-blue-600 disabled:bg-gray-300 disabled:cursor-not-allowed">Next Step</button></div><div class="bg-white rounded-lg p-4 relative"><div class="relative flex items-center py-3 pl-4 pr-4 ml-44 rounded bg-blue-50 border-blue-200"><div class="absolute -left-44 w-40 text-right text-sm font-medium text-gray-700">Prior log-odds</div><div class="flex-1 flex items-center"><div class="flex-1 text-right"><span class="font-mono text-sm font-bold">1</span></div><div class="w-8 flex justify-center"><span class="text-gray-500 font-bold"><span class="text-xs font-normal text-gray-400">vs</span></span></div><div class="flex-1 text-left"><span class="font-mono text-sm font-bold">0</span></div></div></div><div class="mt-6 pt-4 border-t border-gray-300"><div class="relative flex items-center py-3 pl-4 pr-4 ml-44 rounded bg-green-50 mb-2"><div class="absolute -left-44 w-40 text-right text-sm font-medium text-gray-700">Posterior log odds</div><div class="flex-1 flex items-center"><div class="flex-1 text-right"><span class="font-mono text-sm font-bold text-blue-600">1.00</span></div><div class="w-8 flex justify-center"><span class="text-gray-500 font-bold"><span class="text-xs font-normal text-gray-400">vs</span></span></div><div class="flex-1 text-left"><span class="font-mono text-sm font-bold text-blue-600">0.00</span></div></div></div><div class="relative flex items-center py-3 pl-4 pr-4 ml-44 rounded bg-green-50 mb-2"><div class="absolute -left-44 w-40 text-right text-sm font-medium text-gray-700">Posterior odds</div><div class="flex-1 flex items-center"><div class="flex-1 text-right"><span class="font-mono text-sm font-bold text-blue-600">2.000</span></div><div class="w-8 flex justify-center"><span class="text-gray-500 font-bold">:</span></div><div class="flex-1 text-left"><span class="font-mono text-sm font-bold text-blue-600">1.000</span></div></div></div><div class="relative flex items-center py-3 pl-4 pr-4 ml-44 rounded bg-green-50"><div class="absolute -left-44 w-40 text-right text-sm font-medium text-gray-700">Posterior probability</div><div class="flex-1 flex items-center"><div class="flex-1 text-right"><span class="font-mono text-sm font-bold text-blue-600">66.7<!-- -->%</span></div><div class="w-8 flex justify-center"></div><div class="flex-1 text-left"><span class="font-mono text-sm font-bold text-blue-600">33.3<!-- -->%</span></div></div></div></div></div></div></div>
<p>Each step adds surprisals (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>log</mi><mo>⁡</mo><mn>1</mn><mi mathvariant="normal">/</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">\log 1/p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1/</span><span class="mord mathnormal">p</span></span></span></span>) to track evidence.
Last time, we focused on the differences between surprisals to see how much evidence we got for each hypothesis. Our Bayesian detective just keeps adding up these differences.</p>
<p>But the detective could also add up the total surprisal for each hypothesis (green and orange numbers in the above widget), and then compare overall <em>Total surprisal</em>  values. This corresponds to writing KL divergence like this:</p>
<div class="katex-display-wrapper" style="overflow-x:auto;overflow-y:hidden;-webkit-overflow-scrolling:touch;width:100%;margin:1rem 0;text-align:center"><span style="display:inline-block"></span></div>
<p>These two pieces on the right are super important: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>p</mi><mo separator="true">,</mo><mi>q</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H(p,q)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="mclose">)</span></span></span></span> is called cross-entropy and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H(p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mclose">)</span></span></span></span> is entropy. Let&#x27;s build intuition for what they mean.</p>
<h2>Cross-entropy <a id="cross"></a></h2>
<p>Think of cross-entropy as: <strong>how surprised you are on average when seeing data from <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span></span></span></span> while modeling it as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span></span></span></span>?</strong></p>
<p>Explore this in the widget below. The widget shows what happens when our Bayesian detective from the previous chapter keeps flipping her coin. The red dashed line is showing cross-entropy - the expected surprisal of the model <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span></span></span></span> as we keep flipping the coin with bias <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span></span></span></span>. The orange line shows the entropy—this is the expected surprisal when both the model and actual bias are <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span></span></span></span>.
KL divergence is the difference between cross-entropy and entropy. Notice that the cross-entropy line is always above the entropy line (equivalently, KL divergence is always positive).</p>
<p>If you let the widget run, you will also see a blue and a green curve - the actual surprisal measured by our detective in the flipping simulation. We could also say that these curves measure cross-entropy—it&#x27;s the cross-entropy between the <em>empirical distribution</em> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>p</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{p}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">p</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1667em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span></span></span></span> (the actual outcomes of the flips) and the model <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span></span></span></span> (blue curve) or <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span></span></span></span> (green curve). The empirical cross-entropies are tracking the dashed lines due to the <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers">law of large numbers</a>.</p>
<div class="mb-6 p-4 bg-gray-50 rounded-lg"><h4 class="text-center mb-4">Cross-Entropy Simulator</h4><div class="space-y-4"><div class="grid grid-cols-1 md:grid-cols-2 gap-4"><div><label class="block text-sm font-medium text-gray-700 mb-1">True Heads Probability (P): <!-- -->20%</label><input type="range" min="0.01" max="0.99" step="0.01" class="w-full h-2 rounded-lg appearance-none cursor-pointer bg-blue-200" value="0.2"/></div><div><label class="block text-sm font-medium text-gray-700 mb-1">Model Heads Probability (Q): <!-- -->80%</label><input type="range" min="0.01" max="0.99" step="0.01" class="w-full h-2 rounded-lg appearance-none cursor-pointer bg-red-200" value="0.8"/></div></div><div class="text-center"><div class="inline-block"><div data-testid="react-katex"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mn>0.722</mn><mtext> bits/flip</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo separator="true">,</mo><mi>Q</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mn>1.922</mn><mtext> bits/flip</mtext></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
                H(P) &amp;= 0.722\text{ bits/flip} \\
                H(P,Q) &amp;= 1.922\text{ bits/flip}
              \end{align*}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3em;vertical-align:-1.25em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.75em;"><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mclose">)</span></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">Q</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.25em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.75em;"><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">0.722</span><span class="mord text"><span class="mord"> bits/flip</span></span></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">1.922</span><span class="mord text"><span class="mord"> bits/flip</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.25em;"><span></span></span></span></span></span></span></span></span></span></span></span></div></div></div></div><div class="my-4"><div class="space-y-4"><div class="grid grid-cols-1 md:grid-cols-2 gap-4"><div><label class="block text-sm font-medium text-gray-700 mb-1">Number of Flips: <!-- -->200</label><input type="range" min="10" max="500" step="10" class="w-full h-2 rounded-lg appearance-none cursor-pointer bg-gray-200" value="200"/></div><div><label class="block text-sm font-medium text-gray-700 mb-1">Simulation Speed (faster ←→ slower)</label><input type="range" min="10" max="500" step="10" class="w-full h-2 rounded-lg appearance-none cursor-pointer bg-purple-200" value="50"/></div></div><div class="flex justify-center items-center gap-4"><button class="px-4 py-2 rounded-md font-medium text-white bg-blue-500 hover:bg-blue-600">Start</button><button class="px-4 py-2 rounded-md font-medium text-white bg-gray-500 hover:bg-gray-600" disabled="">Reset</button></div></div></div><div class="bg-white rounded-lg"><div class="relative"><div class="h-96 transition-all duration-300 relative"><div class="mb-2"><div class="flex justify-end mb-2"><button class="px-3 py-1 text-xs bg-blue-500 hover:bg-blue-600 text-white rounded transition-colors z-50">🔍+ Zoom In</button></div></div><div class="recharts-responsive-container" style="width:100%;height:100%;min-width:0"></div></div></div></div></div>
<p>Bottom line: <em>Better models are less surprised by the data and have smaller cross-entropy. KL divergence measures how far our model is from the best one.</em></p>
<h2>Entropy</h2>
<p>The term <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo>=</mo><mi>H</mi><mo stretchy="false">(</mo><mi>p</mi><mo separator="true">,</mo><mi>p</mi><mo stretchy="false">)</mo><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msub><mi>p</mi><mi>i</mi></msub><mi>log</mi><mo>⁡</mo><mn>1</mn><mi mathvariant="normal">/</mi><msub><mi>p</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">H(p) = H(p, p) = \sum_{i = 1}^n p_i \log 1 / p_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">p</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.104em;vertical-align:-0.2997em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8043em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1/</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> is a special case of cross-entropy called just plain <em>entropy</em>. It&#x27;s the best possible cross-entropy you can get for distribution <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span></span></span></span>—when you model it perfectly as itself.</p>
<p>Intuitively, entropy tells you how much surprisal or uncertainty is baked into <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span></span></span></span>. Even if you know you&#x27;re flipping a fair coin and hence <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mi>q</mi><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">p = q = \frac12</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.1901em;vertical-align:-0.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>, you still don&#x27;t know which way the coin will land. There&#x27;s inherent uncertainty in that—the outcome still carries surprisal, even if you know the coin&#x27;s bias. This is what entropy measures.</p>
<p>The fair coin&#x27;s entropy is <span></span> bit.
But entropy can get way smaller than 1 bit. If you flip a biased coin where heads are very unlikely—say <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mtext mathvariant="sans-serif">H</mtext><mo>=</mo><mn>0.05</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(\textsf{H} = 0.05)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord text"><span class="mord textsf">H</span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">0.05</span><span class="mclose">)</span></span></span></span>—the entropy of the flip gets close to zero. Makes sense! Sure, if you happen to flip heads, that&#x27;s super surprising (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>log</mi><mo>⁡</mo><mn>1</mn><mi mathvariant="normal">/</mi><mn>0.05</mn><mo>≈</mo><mn>4.32</mn></mrow><annotation encoding="application/x-tex">\log 1/0.05 \approx 4.32</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1/0.05</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">4.32</span></span></span></span>). However, most flips are boringly predictable tails, so the <em>average</em> surprise gets less than 1 bit. You can check in the widget below that <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mo stretchy="false">{</mo><mtext mathvariant="sans-serif">H</mtext><mo>:</mo><mn>0.05</mn><mo separator="true">,</mo><mtext mathvariant="sans-serif">T</mtext><mo>:</mo><mn>0.95</mn><mo stretchy="false">}</mo><mo stretchy="false">)</mo><mo>≈</mo><mn>0.29</mn></mrow><annotation encoding="application/x-tex">H(\{\textsf{H}: 0.05, \textsf{T}: 0.95\}) \approx 0.29</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mopen">({</span><span class="mord text"><span class="mord textsf">H</span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord">0.05</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord text"><span class="mord textsf">T</span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">0.95</span><span class="mclose">})</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.29</span></span></span></span> bits per flip. Entropy hits zero when one outcome has 100% probability.</p>
<p>Entropy can also get way bigger than 1 bit. Rolling a die has entropy <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo><mo>≈</mo><mn>2.6</mn></mrow><annotation encoding="application/x-tex">\log_2(6) \approx 2.6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">6</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">2.6</span></span></span></span> bits. In general, a uniform distribution over <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span> options has entropy <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mi>k</mi></mrow><annotation encoding="application/x-tex">\log_2 k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9386em;vertical-align:-0.2441em"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span>, which is the maximum entropy possible for <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span> options. Makes sense—you&#x27;re most surprised on average when the distribution is, in a sense, most uncertain.</p>
<div class="p-4 bg-gray-50 rounded-lg space-y-4 max-w-2xl mx-auto"><h3 class="text-lg font-semibold text-center text-gray-800">Entropy of die-rolling</h3><div class="flex justify-center overflow-x-auto"><svg width="400" height="250" class="border rounded bg-white min-w-0 max-w-full"><rect x="40" y="20" width="340" height="190" fill="#f9fafb" stroke="#e5e7eb"></rect><g><line x1="40" y1="172" x2="380" y2="172" stroke="#e5e7eb" stroke-dasharray="2,2"></line><text x="30" y="176" text-anchor="end" font-size="10" fill="#6b7280">0.2</text></g><g><line x1="40" y1="134" x2="380" y2="134" stroke="#e5e7eb" stroke-dasharray="2,2"></line><text x="30" y="138" text-anchor="end" font-size="10" fill="#6b7280">0.4</text></g><g><line x1="40" y1="96" x2="380" y2="96" stroke="#e5e7eb" stroke-dasharray="2,2"></line><text x="30" y="100" text-anchor="end" font-size="10" fill="#6b7280">0.6</text></g><g><line x1="40" y1="58" x2="380" y2="58" stroke="#e5e7eb" stroke-dasharray="2,2"></line><text x="30" y="62" text-anchor="end" font-size="10" fill="#6b7280">0.8</text></g><g><line x1="40" y1="20" x2="380" y2="20" stroke="#e5e7eb" stroke-dasharray="2,2"></line><text x="30" y="24" text-anchor="end" font-size="10" fill="#6b7280">1.0</text></g><text x="68.33333333333333" y="230" text-anchor="middle" font-size="12" fill="#374151" font-weight="bold">A</text><text x="124.99999999999999" y="230" text-anchor="middle" font-size="12" fill="#374151" font-weight="bold">B</text><text x="181.66666666666666" y="230" text-anchor="middle" font-size="12" fill="#374151" font-weight="bold">C</text><text x="238.33333333333334" y="230" text-anchor="middle" font-size="12" fill="#374151" font-weight="bold">D</text><text x="294.99999999999994" y="230" text-anchor="middle" font-size="12" fill="#374151" font-weight="bold">E</text><text x="351.66666666666663" y="230" text-anchor="middle" font-size="12" fill="#374151" font-weight="bold">F</text><g><rect x="45.666666666666664" y="178.33333333333334" width="45.333333333333336" height="31.666666666666664" fill="rgb(167, 193.4, 228.6)" stroke="#374151" stroke-width="1" class="cursor-ns-resize hover:opacity-80 transition-opacity"></rect><text x="68.33333333333333" y="173.33333333333334" text-anchor="middle" font-size="10" fill="#374151" font-weight="bold">0.17</text></g><g><rect x="102.33333333333333" y="178.33333333333334" width="45.333333333333336" height="31.666666666666664" fill="rgb(167, 193.4, 228.6)" stroke="#374151" stroke-width="1" class="cursor-ns-resize hover:opacity-80 transition-opacity"></rect><text x="125" y="173.33333333333334" text-anchor="middle" font-size="10" fill="#374151" font-weight="bold">0.17</text></g><g><rect x="158.99999999999997" y="178.33333333333334" width="45.333333333333336" height="31.666666666666664" fill="rgb(167, 193.4, 228.6)" stroke="#374151" stroke-width="1" class="cursor-ns-resize hover:opacity-80 transition-opacity"></rect><text x="181.66666666666663" y="173.33333333333334" text-anchor="middle" font-size="10" fill="#374151" font-weight="bold">0.17</text></g><g><rect x="215.66666666666666" y="178.33333333333334" width="45.333333333333336" height="31.666666666666664" fill="rgb(167, 193.4, 228.6)" stroke="#374151" stroke-width="1" class="cursor-ns-resize hover:opacity-80 transition-opacity"></rect><text x="238.33333333333331" y="173.33333333333334" text-anchor="middle" font-size="10" fill="#374151" font-weight="bold">0.17</text></g><g><rect x="272.3333333333333" y="178.33333333333334" width="45.333333333333336" height="31.666666666666664" fill="rgb(167, 193.4, 228.6)" stroke="#374151" stroke-width="1" class="cursor-ns-resize hover:opacity-80 transition-opacity"></rect><text x="295" y="173.33333333333334" text-anchor="middle" font-size="10" fill="#374151" font-weight="bold">0.17</text></g><g><rect x="329" y="178.33333333333334" width="45.333333333333336" height="31.666666666666664" fill="rgb(167, 193.4, 228.6)" stroke="#374151" stroke-width="1" class="cursor-ns-resize hover:opacity-80 transition-opacity"></rect><text x="351.6666666666667" y="173.33333333333334" text-anchor="middle" font-size="10" fill="#374151" font-weight="bold">0.17</text></g><line x1="40" y1="210" x2="380" y2="210" stroke="#374151" stroke-width="2"></line><line x1="40" y1="20" x2="40" y2="210" stroke="#374151" stroke-width="2"></line><text x="15" y="115" text-anchor="middle" font-size="12" fill="#374151" font-weight="bold" transform="rotate(-90, 15, 115)">Probability</text></svg></div><div class="widget-explanation">Drag any bar up or down to adjust probabilities. Other bars adjust automatically.</div><div class="bg-blue-50 p-4 rounded-lg border border-blue-200"><div class="text-center"><div class="text-sm text-gray-700 mb-2">Entropy</div><div class="text-3xl font-mono font-bold text-blue-600">2.585<!-- --> bits</div><div class="text-xs text-gray-500 mt-1">Maximum possible: <!-- -->2.585<!-- --> bits</div><div class="w-full bg-gray-200 rounded-full h-2 mt-2"><div class="bg-blue-500 h-2 rounded-full transition-all duration-300" style="width:100%"></div></div></div></div><div class="flex flex-wrap gap-2 justify-center"><button class="px-3 py-1 text-sm bg-white border border-gray-300 rounded hover:bg-gray-50 transition-colors">Uniform</button><button class="px-3 py-1 text-sm bg-white border border-gray-300 rounded hover:bg-gray-50 transition-colors">Skewed</button><button class="px-3 py-1 text-sm bg-white border border-gray-300 rounded hover:bg-gray-50 transition-colors">Binary</button><button class="px-3 py-1 text-sm bg-white border border-gray-300 rounded hover:bg-gray-50 transition-colors">Extreme</button></div></div>
<div class="Expand_expand__enJz_"><div class="Expand_header__Q6jAu" style="background-color:#f5f5f5"><div class="Expand_headline__Bgb1v">Example: correct horse battery staple</div><div class="Expand_arrow__LMPES "><svg width="12" height="12" viewBox="0 0 12 12" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M2 4L6 8L10 4" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></div>
<h2>Relative entropy</h2>
<p>KL divergence can be interpreted as the gap between cross-entropy and entropy. It tells us how far your average surprisal (cross-entropy) is from the best possible (entropy).
That&#x27;s why in some communities, people call KL divergence the <em>relative entropy</em> between <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span></span></span></span>. <sup id="ref-footnote-1-1" class="Footnotes_footnote__9N0Ba" tabindex="0" role="button" data-state="closed"><span class="Footnotes_footnote-link__l83hx">1</span></sup></p>
<h2>What&#x27;s next? <a id="next-steps"></a></h2>
<p>We&#x27;re getting the hang of KL divergence, cross-entropy, and entropy! Quick recap:</p>
<p><img src="/problens-web/02-crossentropy/crossentropy_entropy_formula.png" alt="Formula for KL divergence" style="cursor:pointer" class=""/></p>
<p>In the <a href="/03-entropy_properties">next chapter</a>, we will do a recap of what kind of properties these functions have and then we are ready to get to the cool stuff.</p><div class="mt-16 pt-8 border-t border-gray-200"><div class="flex justify-between items-center"><div class="flex-1"><a class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" href="/bayes-bits-brains/01-kl_intro/"><svg class="w-5 h-5 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 19l-7-7 7-7"></path></svg><div class="text-left"><div class="text-sm text-gray-500">Previous</div><div class="font-medium">Bayes &amp; KL divergence</div></div></a></div><div class="flex-1 text-right"><a class="inline-flex items-center justify-end text-blue-600 hover:text-blue-800 transition-colors" href="/bayes-bits-brains/03-entropy_properties/"><div class="text-right"><div class="text-sm text-gray-500">Next</div><div class="font-medium">KL &amp; Entropy properties</div></div><svg class="w-5 h-5 ml-2" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7"></path></svg></a></div></div></div></article></main></div><footer class="mt-4"><div class="max-w-screen-xl mx-auto px-4 sm:px-6 lg:px-8"><div class="pt-4 pb-12" style="margin-left:calc(max((100% - var(--content-width)) / 2 + 1rem, 0rem))"><p class="text-base text-neutral-500"></p></div></div></footer></div><script src="/bayes-bits-brains/_next/static/chunks/webpack-9a476a05d384b821.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:\"$Sreact.strict_mode\"\n4:I[1063,[],\"\"]\n5:I[11483,[],\"\"]\n7:I[95125,[],\"OutletBoundary\"]\na:I[95125,[],\"ViewportBoundary\"]\nc:I[95125,[],\"MetadataBoundary\"]\ne:I[31954,[],\"\"]\n:HL[\"/bayes-bits-brains/_next/static/media/1f3fe8c6df3d47c1-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/bayes-bits-brains/_next/static/media/558ca1a6aa3cb55e-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/bayes-bits-brains/_next/static/css/dbd2812aeb31ae93.css\",\"style\"]\n:HL[\"/bayes-bits-brains/_next/static/css/5eacd01f773eed7f.css\",\"style\"]\n:HL[\"/bayes-bits-brains/_next/static/css/622677e989ec6b9c.css\",\"style\"]\n:HL[\"/bayes-bits-brains/_next/static/css/8870e1553eaf2585.css\",\"style\"]\n:HL[\"/bayes-bits-brains/_next/static/css/13ab9833c5f6a889.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"G9LV8pwmmB789ZWVZT0j8\",\"p\":\"/bayes-bits-brains\",\"c\":[\"\",\"02-crossentropy\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[[\"path\",\"02-crossentropy\",\"c\"],{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/bayes-bits-brains/_next/static/css/dbd2812aeb31ae93.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/bayes-bits-brains/_next/static/css/5eacd01f773eed7f.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"2\",{\"rel\":\"stylesheet\",\"href\":\"/bayes-bits-brains/_next/static/css/622677e989ec6b9c.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"3\",{\"rel\":\"stylesheet\",\"href\":\"/bayes-bits-brains/_next/static/css/8870e1553eaf2585.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"4\",{\"rel\":\"stylesheet\",\"href\":\"/bayes-bits-brains/_next/static/css/13ab9833c5f6a889.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"__variable_871a3d __variable_e566b6\",\"children\":[\"$\",\"$2\",null,{\"children\":[[\"$\",\"body\",null,{\"className\":\"antialiased\",\"children\":\"$L3\"}],\"$undefined\"]}]}]]}],{\"children\":[[\"path\",\"02-crossentropy\",\"c\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L6\",\"$undefined\",null,[\"$\",\"$L7\",null,{\"children\":[\"$L8\",\"$L9\",null]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"qUgkKQrhalg8TCl04B1y0\",{\"children\":[[\"$\",\"$La\",null,{\"children\":\"$Lb\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],[\"$\",\"$Lc\",null,{\"children\":\"$Ld\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$e\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"f:I[79444,[\"683\",\"static/chunks/45233be0-cba42a3c5454822d.js\",\"338\",\"static/chunks/4bbad5ef-2b7eebeba98386a5.js\",\"932\",\"static/chunks/f885ef2c-649d91f122a11dff.js\",\"755\",\"static/chunks/0133708f-65d46efc97e0f93d.js\",\"269\",\"static/chunks/91c6c604-01c8ba6fb8219fe8.js\",\"950\",\"static/chunks/950-e6e0e01ad2983f53.js\",\"390\",\"static/chunks/390-7b8633ed8924eb08.js\",\"582\",\"static/chunks/582-781968895face3a7.js\",\"180\",\"static/chunks/app/%5B...path%5D/page-4c83be7a70b80924.js\"],\"default\"]\n10:I[60544,[\"683\",\"static/chunks/45233be0-cba42a3c5454822d.js\",\"338\",\"static/chunks/4bbad5ef-2b7eebeba98386a5.js\",\"932\",\"static/chunks/f885ef2c-649d91f122a11dff.js\",\"755\",\"static/chunks/0133708f-65d46efc97e0f93d.js\",\"269\",\"static/chunks/91c6c604-01c8ba6fb8219fe8.js\",\"950\",\"static/chunks/950-e6e0e01ad2983f53.js\",\"390\",\"static/chunks/390-7b8633ed8924eb08.js\",\"582\",\"static/chunks/582-781968895face3a7.js\",\"180\",\"static/chunks/app/%5B...path%5D/page-4c83be7a70b80924.js\"],\"default\"]\n11:I[5340,[\"683\",\"static/chunks/45233be0-cba42a3c5454822d.js\",\"338\",\"static/chunks/4bbad5ef-2b7eebeba98386a5.js\",\"932\",\"static/chunks/f885ef2c-649d91f122a11dff.js\",\"755\",\"static/chunks/0133708f-65d46efc97e0f93d.js\",\"269\",\"static/chunks/91c6c604-01c8ba6fb8219fe8.js\",\"950\",\"static/chunks/950-e6e0e01ad2983f53.js\",\"390\",\"static/chunks/390-7b8633ed8924eb08.js\",\"582\",\"static/chunks/582-781968895face3a7.js\",\"180\",\"static/chunks/app/%5B...path%5D/page-4c83be7a70b80924.js\"],\"default\"]\n12:I[47950,[\"683\",\"static/chunks/45233be0-cba42a3c5454822d.js\",\"338\",\"static/chunks/4bbad5ef-2b7eebeba98386a5.js\",\"932\",\"static/chunks/f885ef2c-649d91f122a11dff.js\",\"755\",\"static/chunks/0133708f-65d46efc97e0f93d.js\",\"269\",\"static/chunks/91c6c604-01c8ba6fb8219fe8.js\",\"950\",\"static/chunks/950-e6e0e01ad2983f53.js\",\"390\",\"static/chunks/390-7b8633ed8924eb08.js\",\"582\",\"static/chunks/582-781968895face3a7.js\",\"180\",\"static/chunks/app/%5B...path%5D/page-4c83be7a70b80924.js\"],\"\"]\n3:[\"$\",\"div\",null,{\"className\":\"min-h-screen flex flex-col bg-white\",\"children\":[[\"$\",\"$Lf\",null,{}],[\"$\",\"$L10\",null,{}],[\"$\",\"d"])</script><script>self.__next_f.push([1,"iv\",null,{\"className\":\"flex-1\",\"children\":[[\"$\",\"$L11\",null,{\"className\":\"xl:w-60 lg:w-52 hidden lg:block fixed top-16 bottom-0 pt-16 text-base\",\"style\":{\"left\":\"var(--sidebar-offset)\"}}],[\"$\",\"main\",null,{\"className\":\"Page_main__cWiGN max-w-[var(--content-width)] mx-auto px-4 pt-12 pb-8\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"div\",null,{\"className\":\"min-h-screen flex items-center justify-center bg-gray-50\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-md w-full text-center\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-6xl font-bold text-gray-900 mb-4\",\"children\":\"404\"}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-semibold text-gray-700 mb-4\",\"children\":\"Page Not Found\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600 mb-8\",\"children\":\"The page you're looking for doesn't exist or has been moved.\"}],[\"$\",\"$L12\",null,{\"href\":\"/\",\"className\":\"inline-block bg-blue-600 text-white px-6 py-3 rounded-lg hover:bg-blue-700 transition-colors\",\"children\":\"Go Home\"}]]}]}],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]]}],[\"$\",\"footer\",null,{\"className\":\"mt-4\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-screen-xl mx-auto px-4 sm:px-6 lg:px-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"pt-4 pb-12\",\"style\":{\"marginLeft\":\"calc(max((100% - var(--content-width)) / 2 + 1rem, 0rem))\"},\"children\":[\"$\",\"p\",null,{\"className\":\"text-base text-neutral-500\"}]}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"13:I[10572,[\"683\",\"static/chunks/45233be0-cba42a3c5454822d.js\",\"338\",\"static/chunks/4bbad5ef-2b7eebeba98386a5.js\",\"932\",\"static/chunks/f885ef2c-649d91f122a11dff.js\",\"755\",\"static/chunks/0133708f-65d46efc97e0f93d.js\",\"269\",\"static/chunks/91c6c604-01c8ba6fb8219fe8.js\",\"950\",\"static/chunks/950-e6e0e01ad2983f53.js\",\"390\",\"static/chunks/390-7b8633ed8924eb08.js\",\"582\",\"static/chunks/582-781968895face3a7.js\",\"180\",\"static/chunks/app/%5B...path%5D/page-4c83be7a70b80924.js\"],\"EquationProvider\"]\n14:I[36185,[\"683\",\"static/chunks/45233be0-cba42a3c5454822d.js\",\"338\",\"static/chunks/4bbad5ef-2b7eebeba98386a5.js\",\"932\",\"static/chunks/f885ef2c-649d91f122a11dff.js\",\"755\",\"static/chunks/0133708f-65d46efc97e0f93d.js\",\"269\",\"static/chunks/91c6c604-01c8ba6fb8219fe8.js\",\"950\",\"static/chunks/950-e6e0e01ad2983f53.js\",\"390\",\"static/chunks/390-7b8633ed8924eb08.js\",\"582\",\"static/chunks/582-781968895face3a7.js\",\"180\",\"static/chunks/app/%5B...path%5D/page-4c83be7a70b80924.js\"],\"CitationsProvider\"]\n15:I[44464,[\"683\",\"static/chunks/45233be0-cba42a3c5454822d.js\",\"338\",\"static/chunks/4bbad5ef-2b7eebeba98386a5.js\",\"932\",\"static/chunks/f885ef2c-649d91f122a11dff.js\",\"755\",\"static/chunks/0133708f-65d46efc97e0f93d.js\",\"269\",\"static/chunks/91c6c604-01c8ba6fb8219fe8.js\",\"950\",\"static/chunks/950-e6e0e01ad2983f53.js\",\"390\",\"static/chunks/390-7b8633ed8924eb08.js\",\"582\",\"static/chunks/582-781968895face3a7.js\",\"180\",\"static/chunks/app/%5B...path%5D/page-4c83be7a70b80924.js\"],\"FootnotesProvider\"]\n16:I[66704,[\"683\",\"static/chunks/45233be0-cba42a3c5454822d.js\",\"338\",\"static/chunks/4bbad5ef-2b7eebeba98386a5.js\",\"932\",\"static/chunks/f885ef2c-649d91f122a11dff.js\",\"755\",\"static/chunks/0133708f-65d46efc97e0f93d.js\",\"269\",\"static/chunks/91c6c604-01c8ba6fb8219fe8.js\",\"950\",\"static/chunks/950-e6e0e01ad2983f53.js\",\"390\",\"static/chunks/390-7b8633ed8924eb08.js\",\"582\",\"static/chunks/582-781968895face3a7.js\",\"180\",\"static/chunks/app/%5B...path%5D/page-4c83be7a70b80924.js\"],\"default\"]\n17:I[75154,[\"683\",\"static/chunks/45233be0-cba42a3c5454822d.js\",\"338\",\"static/chunks/4bbad5e"])</script><script>self.__next_f.push([1,"f-2b7eebeba98386a5.js\",\"932\",\"static/chunks/f885ef2c-649d91f122a11dff.js\",\"755\",\"static/chunks/0133708f-65d46efc97e0f93d.js\",\"269\",\"static/chunks/91c6c604-01c8ba6fb8219fe8.js\",\"950\",\"static/chunks/950-e6e0e01ad2983f53.js\",\"390\",\"static/chunks/390-7b8633ed8924eb08.js\",\"582\",\"static/chunks/582-781968895face3a7.js\",\"180\",\"static/chunks/app/%5B...path%5D/page-4c83be7a70b80924.js\"],\"default\"]\n18:I[5770,[\"683\",\"static/chunks/45233be0-cba42a3c5454822d.js\",\"338\",\"static/chunks/4bbad5ef-2b7eebeba98386a5.js\",\"932\",\"static/chunks/f885ef2c-649d91f122a11dff.js\",\"755\",\"static/chunks/0133708f-65d46efc97e0f93d.js\",\"269\",\"static/chunks/91c6c604-01c8ba6fb8219fe8.js\",\"950\",\"static/chunks/950-e6e0e01ad2983f53.js\",\"390\",\"static/chunks/390-7b8633ed8924eb08.js\",\"582\",\"static/chunks/582-781968895face3a7.js\",\"180\",\"static/chunks/app/%5B...path%5D/page-4c83be7a70b80924.js\"],\"default\"]\n19:I[61803,[\"683\",\"static/chunks/45233be0-cba42a3c5454822d.js\",\"338\",\"static/chunks/4bbad5ef-2b7eebeba98386a5.js\",\"932\",\"static/chunks/f885ef2c-649d91f122a11dff.js\",\"755\",\"static/chunks/0133708f-65d46efc97e0f93d.js\",\"269\",\"static/chunks/91c6c604-01c8ba6fb8219fe8.js\",\"950\",\"static/chunks/950-e6e0e01ad2983f53.js\",\"390\",\"static/chunks/390-7b8633ed8924eb08.js\",\"582\",\"static/chunks/582-781968895face3a7.js\",\"180\",\"static/chunks/app/%5B...path%5D/page-4c83be7a70b80924.js\"],\"default\"]\n1a:I[32480,[\"683\",\"static/chunks/45233be0-cba42a3c5454822d.js\",\"338\",\"static/chunks/4bbad5ef-2b7eebeba98386a5.js\",\"932\",\"static/chunks/f885ef2c-649d91f122a11dff.js\",\"755\",\"static/chunks/0133708f-65d46efc97e0f93d.js\",\"269\",\"static/chunks/91c6c604-01c8ba6fb8219fe8.js\",\"950\",\"static/chunks/950-e6e0e01ad2983f53.js\",\"390\",\"static/chunks/390-7b8633ed8924eb08.js\",\"582\",\"static/chunks/582-781968895face3a7.js\",\"180\",\"static/chunks/app/%5B...path%5D/page-4c83be7a70b80924.js\"],\"default\"]\n1b:I[916,[\"683\",\"static/chunks/45233be0-cba42a3c5454822d.js\",\"338\",\"static/chunks/4bbad5ef-2b7eebeba98386a5.js\",\"932\",\"static/chunks/f885ef2c-649d91f122a11dff.js\",\"755\",\"static/chunks/0133708f-65d46efc97e0f93d.js"])</script><script>self.__next_f.push([1,"\",\"269\",\"static/chunks/91c6c604-01c8ba6fb8219fe8.js\",\"950\",\"static/chunks/950-e6e0e01ad2983f53.js\",\"390\",\"static/chunks/390-7b8633ed8924eb08.js\",\"582\",\"static/chunks/582-781968895face3a7.js\",\"180\",\"static/chunks/app/%5B...path%5D/page-4c83be7a70b80924.js\"],\"default\"]\n1c:I[23229,[\"683\",\"static/chunks/45233be0-cba42a3c5454822d.js\",\"338\",\"static/chunks/4bbad5ef-2b7eebeba98386a5.js\",\"932\",\"static/chunks/f885ef2c-649d91f122a11dff.js\",\"755\",\"static/chunks/0133708f-65d46efc97e0f93d.js\",\"269\",\"static/chunks/91c6c604-01c8ba6fb8219fe8.js\",\"950\",\"static/chunks/950-e6e0e01ad2983f53.js\",\"390\",\"static/chunks/390-7b8633ed8924eb08.js\",\"582\",\"static/chunks/582-781968895face3a7.js\",\"180\",\"static/chunks/app/%5B...path%5D/page-4c83be7a70b80924.js\"],\"default\"]\n1d:I[44464,[\"683\",\"static/chunks/45233be0-cba42a3c5454822d.js\",\"338\",\"static/chunks/4bbad5ef-2b7eebeba98386a5.js\",\"932\",\"static/chunks/f885ef2c-649d91f122a11dff.js\",\"755\",\"static/chunks/0133708f-65d46efc97e0f93d.js\",\"269\",\"static/chunks/91c6c604-01c8ba6fb8219fe8.js\",\"950\",\"static/chunks/950-e6e0e01ad2983f53.js\",\"390\",\"static/chunks/390-7b8633ed8924eb08.js\",\"582\",\"static/chunks/582-781968895face3a7.js\",\"180\",\"static/chunks/app/%5B...path%5D/page-4c83be7a70b80924.js\"],\"Footnote\"]\n1e:I[44464,[\"683\",\"static/chunks/45233be0-cba42a3c5454822d.js\",\"338\",\"static/chunks/4bbad5ef-2b7eebeba98386a5.js\",\"932\",\"static/chunks/f885ef2c-649d91f122a11dff.js\",\"755\",\"static/chunks/0133708f-65d46efc97e0f93d.js\",\"269\",\"static/chunks/91c6c604-01c8ba6fb8219fe8.js\",\"950\",\"static/chunks/950-e6e0e01ad2983f53.js\",\"390\",\"static/chunks/390-7b8633ed8924eb08.js\",\"582\",\"static/chunks/582-781968895face3a7.js\",\"180\",\"static/chunks/app/%5B...path%5D/page-4c83be7a70b80924.js\"],\"Footnotes\"]\n1f:I[36185,[\"683\",\"static/chunks/45233be0-cba42a3c5454822d.js\",\"338\",\"static/chunks/4bbad5ef-2b7eebeba98386a5.js\",\"932\",\"static/chunks/f885ef2c-649d91f122a11dff.js\",\"755\",\"static/chunks/0133708f-65d46efc97e0f93d.js\",\"269\",\"static/chunks/91c6c604-01c8ba6fb8219fe8.js\",\"950\",\"static/chunks/950-e6e0e01ad2983f53.js\",\"390\",\"static/chun"])</script><script>self.__next_f.push([1,"ks/390-7b8633ed8924eb08.js\",\"582\",\"static/chunks/582-781968895face3a7.js\",\"180\",\"static/chunks/app/%5B...path%5D/page-4c83be7a70b80924.js\"],\"References\"]\n"])</script><script>self.__next_f.push([1,"6:[\"$\",\"$L13\",null,{\"children\":[\"$\",\"$L14\",null,{\"data\":{},\"children\":[\"$\",\"$L15\",null,{\"children\":[\"$\",\"article\",null,{\"className\":\"Page_article__A_Nud prose prose-neutral max-w-none\",\"children\":[[[\"$\",\"h1\",null,{\"children\":\"KL properties \u0026 (cross-)entropy\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"In this chapter, we'll see how KL divergence can be split into two pieces called \",[\"$\",\"em\",null,{\"children\":\"cross-entropy\"}],\" and \",[\"$\",\"em\",null,{\"children\":\"entropy\"}],\".\"]}],\"\\n\",[\"$\",\"div\",null,{\"className\":\"my-6 rounded-lg overflow-hidden bg-amber-50 border border-amber-200\",\"children\":[[\"$\",\"div\",null,{\"className\":\"px-4 py-3 border-b border-amber-200\",\"children\":[\"$\",\"div\",null,{\"className\":\"font-medium text-base text-amber-800 flex items-center gap-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-lg\",\"children\":\"💡\"}],\"If there is one thing you remember from this chapter...\"]}]}],[\"$\",\"div\",null,{\"className\":\"px-4 py-3 text-base leading-relaxed text-gray-700\",\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"$L16\",null,{\"src\":\"02-crossentropy/crossentropy_entropy_formula.png\",\"alt\":\"Formula for KL divergence\"}],\"\\nCross-entropy measures the average surprisal of model \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"q\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"q\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.625em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.03588em\"},\"children\":\"q\"}]]}]}]]}],\" on data from \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"p\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"p\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.625em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"p\"}]]}]}]]}],\". When \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"p\"}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mi\",null,{\"children\":\"q\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"p = q\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.625em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"p\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.625em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.03588em\"},\"children\":\"q\"}]]}]]}]]}],\", we call it entropy.\"]}]}]]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"KL = cross-entropy - entropy\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Quick refresher: In the \",[\"$\",\"$L17\",null,{\"href\":\"01-kl_intro\",\"children\":\"previous chapter\"}],\", we saw how KL divergence comes from repeatedly using Bayes' theorem with log-space updating:\"]}],\"\\n\",[\"$\",\"$L18\",null,{\"highlightSurprisals\":true}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Each step adds surprisals (\",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"log\"}],[\"$\",\"mo\",null,{\"children\":\"⁡\"}],[\"$\",\"mn\",null,{\"children\":\"1\"}],[\"$\",\"mi\",null,{\"mathvariant\":\"normal\",\"children\":\"/\"}],[\"$\",\"mi\",null,{\"children\":\"p\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\log 1/p\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mop\",\"children\":[\"lo\",[\"$\",\"span\",null,{\"style\":{\"marginRight\":\"0.01389em\"},\"children\":\"g\"}]]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"1/\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"p\"}]]}]}]]}],\") to track evidence.\\nLast time, we focused on the differences between surprisals to see how much evidence we got for each hypothesis. Our Bayesian detective just keeps adding up these differences.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"But the detective could also add up the total surprisal for each hypothesis (green and orange numbers in the above widget), and then compare overall \",[\"$\",\"em\",null,{\"children\":\"Total surprisal\"}],\"  values. This corresponds to writing KL divergence like this:\"]}],\"\\n\",[\"$\",\"$L19\",null,{\"id\":\"cross-entropy-decomp\",\"displayMode\":true,\"math\":\"\\n\\\\underbrace{\\\\sum_{i = 1}^n p_i \\\\log \\\\frac{p_i}{q_i}}_{D(p,q)}\\n\\\\;\\\\;\\\\;=\\\\;\\\\;\\\\;\\n\\\\underbrace{\\\\sum_{i = 1}^n p_i \\\\log \\\\frac{1}{q_i}}_{H(p,q)}\\n\\\\;\\\\;\\\\;-\\\\;\\\\;\\\\;\\n\\\\underbrace{\\\\sum_{i = 1}^n p_i \\\\log \\\\frac{1}{p_i}}_{H(p)}\\n\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"These two pieces on the right are super important: \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"H\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"children\":\"p\"}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\",\"}],[\"$\",\"mi\",null,{\"children\":\"q\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"H(p,q)\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.08125em\"},\"children\":\"H\"}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"p\"}],[\"$\",\"span\",null,{\"className\":\"mpunct\",\"children\":\",\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.03588em\"},\"children\":\"q\"}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}]]}]}]]}],\" is called cross-entropy and \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"H\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"children\":\"p\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"H(p)\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.08125em\"},\"children\":\"H\"}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"p\"}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}]]}]}]]}],\" is entropy. Let's build intuition for what they mean.\"]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":[\"Cross-entropy \",[\"$\",\"a\",null,{\"id\":\"cross\"}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Think of cross-entropy as: \",[\"$\",\"strong\",null,{\"children\":[\"how surprised you are on average when seeing data from \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"p\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"p\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.625em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"p\"}]]}]}]]}],\" while modeling it as \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"q\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"q\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.625em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.03588em\"},\"children\":\"q\"}]]}]}]]}],\"?\"]}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Explore this in the widget below. The widget shows what happens when our Bayesian detective from the previous chapter keeps flipping her coin. The red dashed line is showing cross-entropy - the expected surprisal of the model \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"q\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"q\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.625em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.03588em\"},\"children\":\"q\"}]]}]}]]}],\" as we keep flipping the coin with bias \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"p\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"p\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.625em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"p\"}]]}]}]]}],\". The orange line shows the entropy—this is the expected surprisal when both the model and actual bias are \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"p\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"p\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.625em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"p\"}]]}]}]]}],\".\\nKL divergence is the difference between cross-entropy and entropy. Notice that the cross-entropy line is always above the entropy line (equivalently, KL divergence is always positive).\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"If you let the widget run, you will also see a blue and a green curve - the actual surprisal measured by our detective in the flipping simulation. We could also say that these curves measure cross-entropy—it's the cross-entropy between the \",[\"$\",\"em\",null,{\"children\":\"empirical distribution\"}],\" \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mover\",null,{\"accent\":\"true\",\"children\":[[\"$\",\"mi\",null,{\"children\":\"p\"}],[\"$\",\"mo\",null,{\"children\":\"^\"}]]}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\hat{p}\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.8889em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord accent\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.6944em\"},\"children\":[[\"$\",\"span\",null,{\"style\":{\"top\":\"-3em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"3em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"p\"}]]}],[\"$\",\"span\",null,{\"style\":{\"top\":\"-3em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"3em\"}}],[\"$\",\"span\",null,{\"className\":\"accent-body\",\"style\":{\"left\":\"-0.1667em\"},\"children\":[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"^\"}]}]]}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.1944em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]}]]}],\" (the actual outcomes of the flips) and the model \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"q\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"q\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.625em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.03588em\"},\"children\":\"q\"}]]}]}]]}],\" (blue curve) or \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"p\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"p\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.625em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"p\"}]]}]}]]}],\" (green curve). The empirical cross-entropies are tracking the dashed lines due to the \",[\"$\",\"$L17\",null,{\"href\":\"https://en.wikipedia.org/wiki/Law_of_large_numbers\",\"children\":\"law of large numbers\"}],\".\"]}],\"\\n\",[\"$\",\"$L1a\",null,{}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Bottom line: \",[\"$\",\"em\",null,{\"children\":\"Better models are less surprised by the data and have smaller cross-entropy. KL divergence measures how far our model is from the best one.\"}]]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Entropy\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The term \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"H\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"children\":\"p\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mi\",null,{\"children\":\"H\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"children\":\"p\"}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\",\"}],[\"$\",\"mi\",null,{\"children\":\"p\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"msubsup\",null,{\"children\":[[\"$\",\"mo\",null,{\"children\":\"∑\"}],[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"i\"}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mn\",null,{\"children\":\"1\"}]]}],[\"$\",\"mi\",null,{\"children\":\"n\"}]]}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"p\"}],[\"$\",\"mi\",null,{\"children\":\"i\"}]]}],[\"$\",\"mi\",null,{\"children\":\"log\"}],[\"$\",\"mo\",null,{\"children\":\"⁡\"}],[\"$\",\"mn\",null,{\"children\":\"1\"}],[\"$\",\"mi\",null,{\"mathvariant\":\"normal\",\"children\":\"/\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"p\"}],[\"$\",\"mi\",null,{\"children\":\"i\"}]]}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"H(p) = H(p, p) = \\\\sum_{i = 1}^n p_i \\\\log 1 / p_i\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.08125em\"},\"children\":\"H\"}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"p\"}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.08125em\"},\"children\":\"H\"}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"p\"}],[\"$\",\"span\",null,{\"className\":\"mpunct\",\"children\":\",\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"p\"}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1.104em\",\"verticalAlign\":\"-0.2997em\"}}],[\"$\",\"span\",null,{\"className\":\"mop\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mop op-symbol small-op\",\"style\":{\"position\":\"relative\",\"top\":\"0em\"},\"children\":\"∑\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.8043em\"},\"children\":[[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.4003em\",\"marginLeft\":\"0em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"i\"}],[\"$\",\"span\",null,{\"className\":\"mrel mtight\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"1\"}]]}]}]]}],[\"$\",\"span\",null,{\"style\":{\"top\":\"-3.2029em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"n\"}]}]]}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.2997em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"p\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3117em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"0em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"i\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mop\",\"children\":[\"lo\",[\"$\",\"span\",null,{\"style\":{\"marginRight\":\"0.01389em\"},\"children\":\"g\"}]]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"1/\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"p\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3117em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"0em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"i\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]]}]]}],\" is a special case of cross-entropy called just plain \",[\"$\",\"em\",null,{\"children\":\"entropy\"}],\". It's the best possible cross-entropy you can get for distribution \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"p\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"p\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.625em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"p\"}]]}]}]]}],\"—when you model it perfectly as itself.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Intuitively, entropy tells you how much surprisal or uncertainty is baked into \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"p\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"p\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.625em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"p\"}]]}]}]]}],\". Even if you know you're flipping a fair coin and hence \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"p\"}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mi\",null,{\"children\":\"q\"}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mfrac\",null,{\"children\":[[\"$\",\"mn\",null,{\"children\":\"1\"}],[\"$\",\"mn\",null,{\"children\":\"2\"}]]}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"p = q = \\\\frac12\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.625em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"p\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.625em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.03588em\"},\"children\":\"q\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1.1901em\",\"verticalAlign\":\"-0.345em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mopen nulldelimiter\"}],[\"$\",\"span\",null,{\"className\":\"mfrac\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.8451em\"},\"children\":[[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.655em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"3em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"2\"}]}]}]]}],[\"$\",\"span\",null,{\"style\":{\"top\":\"-3.23em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"3em\"}}],[\"$\",\"span\",null,{\"className\":\"frac-line\",\"style\":{\"borderBottomWidth\":\"0.04em\"}}]]}],[\"$\",\"span\",null,{\"style\":{\"top\":\"-3.394em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"3em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"1\"}]}]}]]}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.345em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"mclose nulldelimiter\"}]]}]]}]]}]]}],\", you still don't know which way the coin will land. There's inherent uncertainty in that—the outcome still carries surprisal, even if you know the coin's bias. This is what entropy measures.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The fair coin's entropy is \",[\"$\",\"$L19\",null,{\"math\":\"H(\\\\{\\\\textsf{H: }1/2, \\\\textsf{T: }1/2\\\\}) = \\\\frac12\\\\cdot \\\\log2 +  \\\\frac12\\\\cdot \\\\log2 = 1\"}],\" bit.\\nBut entropy can get way smaller than 1 bit. If you flip a biased coin where heads are very unlikely—say \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"p\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mtext\",null,{\"mathvariant\":\"sans-serif\",\"children\":\"H\"}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mn\",null,{\"children\":\"0.05\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"p(\\\\textsf{H} = 0.05)\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"p\"}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord text\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord textsf\",\"children\":\"H\"}]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"0.05\"}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}]]}]]}]]}],\"—the entropy of the flip gets close to zero. Makes sense! Sure, if you happen to flip heads, that's super surprising (\",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"log\"}],[\"$\",\"mo\",null,{\"children\":\"⁡\"}],[\"$\",\"mn\",null,{\"children\":\"1\"}],[\"$\",\"mi\",null,{\"mathvariant\":\"normal\",\"children\":\"/\"}],[\"$\",\"mn\",null,{\"children\":\"0.05\"}],[\"$\",\"mo\",null,{\"children\":\"≈\"}],[\"$\",\"mn\",null,{\"children\":\"4.32\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\log 1/0.05 \\\\approx 4.32\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mop\",\"children\":[\"lo\",[\"$\",\"span\",null,{\"style\":{\"marginRight\":\"0.01389em\"},\"children\":\"g\"}]]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"1/0.05\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"≈\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6444em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"4.32\"}]]}]]}]]}],\"). However, most flips are boringly predictable tails, so the \",[\"$\",\"em\",null,{\"children\":\"average\"}],\" surprise gets less than 1 bit. You can check in the widget below that \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"H\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"{\"}],[\"$\",\"mtext\",null,{\"mathvariant\":\"sans-serif\",\"children\":\"H\"}],[\"$\",\"mo\",null,{\"children\":\":\"}],[\"$\",\"mn\",null,{\"children\":\"0.05\"}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\",\"}],[\"$\",\"mtext\",null,{\"mathvariant\":\"sans-serif\",\"children\":\"T\"}],[\"$\",\"mo\",null,{\"children\":\":\"}],[\"$\",\"mn\",null,{\"children\":\"0.95\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"}\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}],[\"$\",\"mo\",null,{\"children\":\"≈\"}],[\"$\",\"mn\",null,{\"children\":\"0.29\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"H(\\\\{\\\\textsf{H}: 0.05, \\\\textsf{T}: 0.95\\\\}) \\\\approx 0.29\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.08125em\"},\"children\":\"H\"}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"({\"}],[\"$\",\"span\",null,{\"className\":\"mord text\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord textsf\",\"children\":\"H\"}]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\":\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.8889em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"0.05\"}],[\"$\",\"span\",null,{\"className\":\"mpunct\",\"children\":\",\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord text\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord textsf\",\"children\":\"T\"}]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\":\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"0.95\"}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\"})\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"≈\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6444em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"0.29\"}]]}]]}]]}],\" bits per flip. Entropy hits zero when one outcome has 100% probability.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Entropy can also get way bigger than 1 bit. Rolling a die has entropy \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"log\"}],[\"$\",\"mo\",null,{\"children\":\"⁡\"}]]}],[\"$\",\"mn\",null,{\"children\":\"2\"}]]}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mn\",null,{\"children\":\"6\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}],[\"$\",\"mo\",null,{\"children\":\"≈\"}],[\"$\",\"mn\",null,{\"children\":\"2.6\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\log_2(6) \\\\approx 2.6\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mop\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mop\",\"children\":[\"lo\",[\"$\",\"span\",null,{\"style\":{\"marginRight\":\"0.01389em\"},\"children\":\"g\"}]]}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.207em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.4559em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"2\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.2441em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"6\"}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"≈\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6444em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"2.6\"}]]}]]}]]}],\" bits. In general, a uniform distribution over \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"k\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"k\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.03148em\"},\"children\":\"k\"}]]}]}]]}],\" options has entropy \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"log\"}],[\"$\",\"mo\",null,{\"children\":\"⁡\"}]]}],[\"$\",\"mn\",null,{\"children\":\"2\"}]]}],[\"$\",\"mi\",null,{\"children\":\"k\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\log_2 k\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.9386em\",\"verticalAlign\":\"-0.2441em\"}}],[\"$\",\"span\",null,{\"className\":\"mop\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mop\",\"children\":[\"lo\",[\"$\",\"span\",null,{\"style\":{\"marginRight\":\"0.01389em\"},\"children\":\"g\"}]]}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.207em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.4559em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"2\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.2441em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.03148em\"},\"children\":\"k\"}]]}]}]]}],\", which is the maximum entropy possible for \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"k\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"k\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.03148em\"},\"children\":\"k\"}]]}]}]]}],\" options. Makes sense—you're most surprised on average when the distribution is, in a sense, most uncertain.\"]}],\"\\n\",[\"$\",\"$L1b\",null,{\"numCategories\":6,\"title\":\"Entropy of die-rolling\"}],\"\\n\",[\"$\",\"$L1c\",null,{\"headline\":\"Example: correct horse battery staple\",\"children\":[[\"$\",\"p\",null,{\"children\":[\"Here's an example. Let's say there are about \",[\"$\",\"$L19\",null,{\"math\":\"2^{11}\"}],\" English words that can be described as 'common'. If you generate uniformly four such common words and make your password the concatenation of them, the total entropy of your password is thus going to be \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mn\",null,{\"children\":\"44\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"44\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6444em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"44\"}]]}]}]]}],\" bits. That's because entropy is a special case of cross-entropy and is thus additive.\"]}],[\"$\",\"p\",null,{\"children\":[\"Having a uniform distribution with 44 bits of entropy is just a different way of saying that we have a uniform distribution with \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"msup\",null,{\"children\":[[\"$\",\"mn\",null,{\"children\":\"2\"}],[\"$\",\"mn\",null,{\"children\":\"44\"}]]}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"2^{44}\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.8141em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"2\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.8141em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-3.063em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"44\"}]}]}]]}]}]}]}]}]]}]]}]}]]}],\" possible outcomes.\\nThis comic wisely teaches us that this many possibilities make it a pretty secure password! Even if an adversary knows how we generated it, cracking it means they have to check about \",[\"$\",\"$L19\",null,{\"math\":\"2^{44}\"}],\" passwords.\"]}],[\"$\",\"a\",null,{\"href\":\"https://xkcd.com/936/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"$\",\"img\",null,{\"src\":\"02-crossentropy/password.png\",\"alt\":\"password\",\"style\":{\"cursor\":\"pointer\"}}]}]]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Relative entropy\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"KL divergence can be interpreted as the gap between cross-entropy and entropy. It tells us how far your average surprisal (cross-entropy) is from the best possible (entropy).\\nThat's why in some communities, people call KL divergence the \",[\"$\",\"em\",null,{\"children\":\"relative entropy\"}],\" between \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"p\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"p\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.625em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"p\"}]]}]}]]}],\" and \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"q\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"q\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.625em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.03588em\"},\"children\":\"q\"}]]}]}]]}],\". \",[\"$\",\"$L1d\",null,{\"children\":\"Way better name than 'KL divergence' if you ask me. But 'KL divergence' is what most people use, so I guess we're stuck with it. \"}]]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":[\"What's next? \",[\"$\",\"a\",null,{\"id\":\"next-steps\"}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"We're getting the hang of KL divergence, cross-entropy, and entropy! Quick recap:\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"$L16\",null,{\"src\":\"02-crossentropy/crossentropy_entropy_formula.png\",\"alt\":\"Formula for KL divergence\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"In the \",[\"$\",\"$L17\",null,{\"href\":\"03-entropy_properties\",\"children\":\"next chapter\"}],\", we will do a recap of what kind of properties these functions have and then we are ready to get to the cool stuff.\"]}]],[\"$\",\"$L1e\",null,{\"headerLevel\":0}],[\"$\",\"$L1f\",null,{}],[\"$\",\"div\",null,{\"className\":\"mt-16 pt-8 border-t border-gray-200\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex justify-between items-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[\"$\",\"$L12\",null,{\"href\":\"/01-kl_intro/\",\"className\":\"inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors\",\"children\":[[\"$\",\"svg\",null,{\"className\":\"w-5 h-5 mr-2\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"viewBox\":\"0 0 24 24\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"strokeWidth\":2,\"d\":\"M15 19l-7-7 7-7\"}]}],[\"$\",\"div\",null,{\"className\":\"text-left\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-sm text-gray-500\",\"children\":\"Previous\"}],[\"$\",\"div\",null,{\"className\":\"font-medium\",\"children\":\"Bayes \u0026 KL divergence\"}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"flex-1 text-right\",\"children\":[\"$\",\"$L12\",null,{\"href\":\"/03-entropy_properties/\",\"className\":\"inline-flex items-center justify-end text-blue-600 hover:text-blue-800 transition-colors\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-right\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-sm text-gray-500\",\"children\":\"Next\"}],[\"$\",\"div\",null,{\"className\":\"font-medium\",\"children\":\"KL \u0026 Entropy properties\"}]]}],[\"$\",\"svg\",null,{\"className\":\"w-5 h-5 ml-2\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"viewBox\":\"0 0 24 24\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"strokeWidth\":2,\"d\":\"M9 5l7 7-7 7\"}]}]]}]}]]}]}]]}]}]}]}]\n"])</script><script>self.__next_f.push([1,"b:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n8:null\n"])</script><script>self.__next_f.push([1,"9:null\nd:[[\"$\",\"title\",\"0\",{\"children\":\"Bayes, bits \u0026 brains\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Bayes, bits \u0026 brains\"}]]\n"])</script></body></html>