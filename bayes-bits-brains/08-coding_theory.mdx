# Coding Theory

Here's one of my favorite science stories. Norbert Wiener was a professor at MIT, known for roaming the corridors and stopping in offices of other researchers. Once, he stopped in the office of Robert Fano, said: "You know, information is entropy", and walked out. Fano started to think about what that meant and a few months later, on a train ride to a meeting, he finally worked it out. At that meeting, a guy named Claude Shannon presented an early version of what later became one of the most cited papers, ever - [The mathematical theory of communication](https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication). The topic of the paper was pretty much the same as what Fano worked out: What does the formula for entropy has to do with storing data on a computer. Shannon and Fano hitted it off the next day and information theory was born. 

I will explain this connection and we will see what intuitions about large language models it gives us. 


<KeyTakeaway>
Entropy is the unbeatable lower bound for lossless compression, if we treat letters independently.  
</KeyTakeaway>


## Code intro
Say we've got a long DNA string built from letters $\{\mathsf{A},\mathsf{C},\mathsf{G},\mathsf{T}\}$. We want to store it using as little disk space as possible. Here's a simple plan: We assign a binary code for each letter and encode the string using that code. 

For example, we can use <Math math = "\mathsf{A} \rightarrow \textsf{00}, \mathsf{C} \rightarrow \textsf{01}, \mathsf{G} \rightarrow \textsf{10}, \mathsf{T} \rightarrow \textsf{11} " />. The string gets stored using just 2 bits per letter. Done! 

Can we do better? Sometimes, yes! Here's a riddle: In the following widget, try to build a code that encodes the following string <Math math = "\mathsf{AACATACG}" /> with just 1.75 bits per letter on average. 

While playing, notice that whenever you use a short code name to a letter, e.g. $\mathsf{A} \rightarrow \textsf{0}$, no other letter can get a code name starting with the same code name $\textsf{0}$. The reason is that otherwise, the resulting code wouldn't sometimes be decodable. E.g., if you use code names <Math  math = "\mathsf{A} \rightarrow \textsf{0}" /> and <Math  math = "\mathsf{C} \rightarrow \textsf{00}" />, the message "<Math  math = "\textsf{00}" />" has two meanings. 

<BuildYourOwnCodeWidget /> 



.

.

SPOILER


.

.

Here's the solution: <Math math = "\mathsf{A} \rightarrow \textsf{0}, \mathsf{C} \rightarrow \textsf{10}, \mathsf{G} \rightarrow \textsf{110}, \mathsf{T} \rightarrow \textsf{111} " />. 
Since the letter frequencies in the text are <Math math = "\frac12, \frac14, \frac18, \frac18"/>, this encoding only uses 
<Math displayMode={true} id = "code-example" math = "\frac12 \cdot 1 + \frac14 \cdot 2 + \frac18 \cdot 3 + \frac18 \cdot 3 = 1.75" />
bits per letter.  

OK, using shorter codes for more frequent letters makes sense, but what's the best way to do this? Coding theory knows the answer. Roughly speaking, it tells us to __try to give a letter with frequency $p_i$ a code-name of length $\log 1/p_i$__. 

For example, looking at <EqRef id = "code-example"/>, you can rewrite the left-hand side as: 
<Math displayMode={true} id = "code-example2" math = " \frac12 \cdot \log \frac{1}{1/2} + \frac14 \cdot \log \frac{1}{1/4} + \frac18 \cdot \log \frac{1}{1/8} + \frac18 \cdot \log\frac{1}{1/8}. " />
Every letter with frequency $p$ got code-name of length exactly $\log 1/p$! Notice that for general strings of length $N$, the length of codes satisfying the $p \rightarrow \log 1/p$ rule is this: 
<Math displayMode={true} math = "N \cdot \sum_{i = 1}^k p_i \cdot \log \frac{1}{p_i} = N \cdot H(p)." />
That is, if you manage to construct a code with $p \rightarrow \log 1/p$, you spend $H(p)$ bits per letter on average. 

If we implement our $p \rightarrow \log 1/p$-rule-of-thumb with the most direct algorithm possible, we get what's known as Shannon's code. This code sorts the letter-frequencies down from the largest one, and assigns to each letter of frequency $p$ a code of length $\lceil \log \frac{1}{p} \rceil$ (we round up because we can't have a code name with 1.5 bits). Here it is for English alphabet:

<ShannonCodeWidget />

You can notice that the number of bits per letter (also called _rate_) is a bit north of the entropy of the underlying distribution. This is because of the rounding $\log 1/p \rightarrow \lceil \log 1/p \rceil$. For example, if there is a letter with frequency $p = 0.49$, we would like to give it a code name of length $\log \frac{1}{0.49} \approx 1.03$. Too bad that code names have to be integers and Shannon's code assigns it a code name of length $2$. As a general rule, Shannon's code can use up to 1 bit per letter more than what's the entropy. <Footnote>Try to work out the details of what's happening in Shannon's code. Especially: Why does Shannon's algorithm of assigning letters never runs out of available code names? </Footnote>

Unfortunately, there are situations where Shannon's code is really almost 1 bit worse than the entropy. Take a string that uses just two characters $\mathsf{A}, \mathsf{B}$, but the frequency of $\mathsf{B}$ is close to zero. Then, the entropy is close to zero, but Shannon can't do better than 1 bit per letter. 


### The source coding theorem <a id="source-coding"></a>

The [source coding theorem](https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem) is the most important theorem in coding theory. It basically says that on one hand, there are codes that get very close to the entropy bound. On the other hand, you can't beat that bound. 

Here's the setup of the theorem more precisely. We model texts like this: There is a source that has a distribution over letters. This source keeps sampling letters independently, and sends them to us. This goes on for a long time, say $n$ steps; our goal is to save some binary string to the memory so that later on, we can recover from that string what $n$ letters the source sent. 

This setup with the source sampling independent letters models that we only want to think about the problem of giving code names to letters based on frequencies. We are not trying to use the _correlations_ like '$\mathsf{th}$ often follows by $\mathsf{e}$' to get better compression.  

The source coding theorem says that first, there's a way to store the emitted string, using close to $H(p)$ bits per letter. We have come close to that bound above - Shannon's code is only 1 bit worse than entropy. But with more complicated codes, we can even get rid of this 1-bit slack (at the expense that the code is no longer as simple as mapping single letters to code names). 

<Expand advanced={true} headline="More on better codes"><a id = "better"></a>

Think of constructing Shannon's code not for every letter, but for every _pair of letters_ (i.e., the alphabet grows from $26$ to $26^2$). If Shannon's code loses 1 bit per encoded letter-pair, it means it loses $1/2$ bits per actual letter! Doing this trick not for a pair of letters but a tuple of several letters can bring us arbitrarily close to a code that spends $H(p)$ bits per letter. 

Doing this is not very practical, since the alphabet gets very larger pretty soon ($26^2$ for pairs, $26^3$ for triples, ...). Check out [arithmetic coding](https://en.wikipedia.org/wiki/Arithmetic_coding) for a practical code approaching the entropy limit. 

</Expand>

The second part of the theorem says that we can't beat the rate $H(p)$. Details for that in the expand box. 

<Expand advanced = {true} headline = "Beating entropy is impossible">
I will prove that codes that simply map each letter to a code-name can't beat $H(p)$. 
To see why, we will have to understand [Kraft's inequality](https://en.wikipedia.org/wiki/Kraft%E2%80%93McMillan_inequality). This inequality says that if we work with alphabet with $k$ letters and we build a code with words of lengths $\ell_1, \dots, \ell_k$, then it has to be the case that 
<Math id = "kraft" displayMode = {true} math = "\sum_{i = 1}^k 2^{- \ell_i} \le 1. " />

To understand this inequality, go back to widgets above - whenever we give a code-name to a letter, like <Math math="\mathsf{e} \rightarrow \textsf{000}" />, we can no longer use codewords that start with <Math math="\textsf{000}" /> for other letters, since that would create clashes. The node with <Math math="\textsf{000}" /> has to be a _leaf_. Some leaves 'take more space' than others - for example, using code name of <Math math="\textsf{0}" /> intuitively kills off half of the space of possibilities. 

A bit more formally, imagine continuing the full binary tree up to some very large depth $N$. Then a code word of length $\ell_i$ is above <Math math="2^{N - \ell_i}" /> of nodes at depth $N$. Since different code words cover disjoint intervals of depth-$N$ nodes, and there are $2^N$ nodes at depth $N$, it has to be the case for any valid code that 
<Math displayMode={true} math="\sum_{i = 1}^k 2^{N - \ell_i} \le 2^N" />
Divide by $2^N$ and you get Kraft's inequality <EqRef id = "kraft"/>. 

I actually like to think about Kraft's inequality as equality. Why? Well, if your code is such that the left-hand side is really smaller than $1$, then you are stupid. <Footnote>You can notice that Shannon's code above is 'stupid' since for English, it leaves some space at the right of the binary tree. In fact, Shannon's code is useful mostly for didactical reasons and in practice you would construct the code by [Huffman's algorithm](https://en.wikipedia.org/wiki/Huffman_coding). </Footnote> In the widget below, you can see how codes can be iteratively improved to get equality. 

<KraftInequalityWidget />


The reason why this is helpful is that I like to think about the numbers <Math math="q_i = 2^{-\ell_i}" /> as some kind of idealized probabilities. Widget above shows that we can pretty much assume that the numbers $q_i$ always sum up to 1. You can think about the numbers $q_i$ as the probability distribution _implied_ by your code-name lengths $\ell_i$. Intuitively, the code is optimized for the distribution $q$, not $p$. 

This setup with $p$ and $q$ is little bit like our discussions in the first two chapters. In fact, let's write down the fact that KL divergence between $p$ and $q$ is always nonnegative. Let's write it down in the form $H(p, q) \ge H(p)$:
<Math displayMode = {true} math="\sum_{i = 1}^k p_i \log \frac{1}{q_i} \ge \sum_{i = 1}^k p_i \log \frac{1}{p_i}" />
Plugging in <Math math="q_i = 2^{-\ell_i}" />, we get
<Math displayMode = {true} math="\sum_{i = 1}^k p_i \ell_i \ge H(p)." />
That is, the average code-name length is at least $H(p)$. This works for any code, hence the second part of [Shannon's source coding theorem](https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem). 
</Expand>


## Prediction = Compression

Using the fact that good codes can achieve a rate close to entropy can give us a lot of intuition! 

Let's go through the terms in our dictionary and see how we can interpret them using the coding-theory language. 

We defined the _surprisal_ $\log 1/p$ as a way of measuring how surprised we are given an outcome that happens with probability $p$. But equivalently, it's also how many bits you need to store the outcome in memory, using the optimal code. 

We introduced entropy as the "average surprisal" of a distribution. But equivalently, it's the rate of how many bits on average we need to store outcomes sampled from a distribution $p$: We can interpret the formula $\sum_{i = 1}^n p_i \log 1/p_i$ as "The $i$-th symbol comes with frequency $p_i$ and whenever it comes, we need $\log 1/p_i$ bits to store it". 

The cross-entropy is very similar; it's how many bits we need when data comes from $p$ but we store them using the code that's optimized for a different distribution $q$. Finally, KL divergence (relative entropy) is how much worse our mismatched code is compared to the optimal one. 

Here it is again, more concisely in a table: 

<table>
  <thead>
    <tr>
      <th style={{ minWidth: "7rem" }}>Concept</th>
      <th>Formula</th>
      <th>Bayesian detective</th>
      <th>Compression</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Surprisal</td>
      <td><Math math="\log \frac{1}{p}" /></td>
      <td>Surprise of an outcome</td>
      <td>Bits to store a letter</td>
    </tr>
    <tr>
      <td>Entropy</td>
      <td><Math math="\sum p_i \log \frac{1}{p_i}" /></td>
      <td>Expected surprisal</td>
      <td>Average bits per letter</td>
    </tr>
    <tr>
      <td>Cross-entropy</td>
      <td><Math math="\sum p_i \log \frac{1}{q_i}" /></td>
      <td>Expected surprisal of model q</td>
      <td>Average bits per letter for code optimized for q</td>
    </tr>
    <tr>
      <td>KL divergence</td>
      <td><Math math="\sum p_i \log \frac{p_i}{q_i}" /></td>
      <td>Additional surprise from p ≠ q</td>
      <td>Additional storage from p ≠ q</td>
    </tr>
  </tbody>
</table>


## LLM compression

So far, all our discussion was about encoding each letter independently of its context. The real fun starts when we want to compress a given text as much as possible, period. Now we definitely want to use the fact that $\textsf{th}$ usually follows by $\textsf{e}$ as much as possible. 

I want to explain a hypothetical compression algorithm based on an LLM. 

First, a technicality we have to go through quickly. Although [we discussed](04-mle#llm) that LLMs are predicting the next letter, that's a simplification. Before training any LLM, we split all texts into _tokens_ and the input and output of LLMs consists of tokens, not letters. There is no fixed definition of what a token is, but for example GPT-2 works with around $50\,000$ different tokens. That's enough to cover all frequent syllables, medium-length words, common names, and special symbols. 

LLMs can be used to compress text. Here's how: We will assume that both the compression and the decompression algorithm have access to the same LLM, say GPT-2. The compression algorithm runs GPT-2 on the input text; for each position $i$ in the text, GPT-2 generates a probability distribution $q_{LLM}(t_i)$ over the $i$ token, based on the tokens $t_1, \dots, t_{i-1}$. We can use a code like Shannon's code to turn this distribution to a code assignment. For example, if GPT-2 predicts that $\textsf{the}$ has 50% probability of coming next, the Shannon's code would assign the code $\textsf{0}$ to $\textsf{the}$. We encode the _actual_ next token $t_i$ using this code. 

That is, our compression algorithm uses _a different code for each token_, each code corresponding to the distribution over that token predicted by GPT-2. The decoding algorithm works analogously: For each token, it uses GPT-2 & Shannon's code to construct the code used to read the next token code. 

The overall length of the encoded text is <Math math = "\sum_{i = 1}^n \lceil \log \frac{1}{q_{LLM}(t_i)} \rceil "/> if we use Shannon's code. In the next widget, you can see how it works. 

<GPT2CompressionWidget />

Our compression algorithm is not very practical, it's very slow. More importantly, both parties also have to have GPT-2 stored on their computer, so we are getting some compression only if the text is much larger than the size of GPT-2, which is in gigabytes. 

But here's the punchline. [Using more complicated codes](08-coding_theory#better), we can get rid of the annoying $\lceil \rceil$ and encode the input text with length very close to 

<Math displayMode={true} math = "\sum_{i = 1}^n \log \frac{1}{q_{LLM}(t_i)}. "/> 

This is exactly the cross-entropy loss used to train neural nets, as [we discussed earlier](04-mle#cross-loss). We are now getting an equivalent definition of what neural nets are being trained for! __LLMs are trained to compress the Internet as much as possible__!  

I think that this is a powerful intuition that can help us understand where the knowledge / intelligence of LLMs is coming from. If you want to compress text efficiently, you have to learn the patterns in it so that you can recreate the same text just by storing a few hints about what it was about. To be good in compression, you have to learn that $\textsf{th}$ often follows by $\textsf{e}$, texts about $\textsf{Paris}$ often involve $\textsf{France}$, and what kinds of talking points might a virtue-ethicist use to critique conseqentialist ethics. 

This is a complementary intuition to our intuition that being good in the text-prediction game requires knowledge. Indeed, prediction and compression are two sides of the same coin! 

## Cracking riddles <a id = "compression"></a>

Let's go back to our Wikipedia compression riddle, the last riddle we haven't discussed yet. 

<RiddleExplanation id="wikipedia">
Remember, [our Wikipedia riddle](00-riddles#wikipedia) was about Hutter's compression challenge: How much can we compress a single 1GB file of English Wikipedia. Hutter is an AI researcher and he started his challenge because he understands very well the topic of this chapter - if you can compress text, you can also predict it, and that implies you have to have relevant knowledge /intelligence.  

The honest answer to what's the best way to compress Wikipedia is that we don't know (see the following [Kolmogorov complexity](09-kolmogorov) chapter), but we at least broadly understand how well different algorithms compress English text. 

There are more approaches to compress text files. Let's go through them and you can see in the next widget how they fare for various types of data. 

- _Baseline_: The stadard way to store text files is UTF-8. Simplifying a bit, this format store each letter using 8 bits. <Footnote>Why is it lying? [UTF-8](https://en.wikipedia.org/wiki/UTF-8) itself is a beautiful example of good engineering inspired by coding theory. There are around 100 characters (English letters, digits) that are stored using 8 bits, fancier letters from reasonable alphabets are stored using 16 bits, and emojis like 😀 or hieroglyphs like 𓀀 take 32 bits. Classic coding theory—rare stuff gets longer codes. But English Wiki is mostly standard English letters, so 8 bits per character it is. </Footnote>

- _Optimal letter code_: Treat letters as independent, we can encode them using $H(p)$ bits per letter on average, where $p$ is the distribution of English letter frequencies. [In the Shannon's coding widget above](../02-crossentropy#construction), we saw that the entropy is around 4.2 bits. So, we can shrink the file by almost __2x__ just by using that different letters have different frequencies. 

- _Zipping_: Standard compression algorithms like those used in ZIP implement dictionaries to catch patterns that are often reused in the text. They can compress English text up to a factor around __3x__.  

- _LLMs_: Large Language Models are trained to predict text, hence they compress it. If we disregard the fact that technically speaking, LLM size should count toward the size of the compressed text, they compress English text around __7x__ for old LLMs like GPT-2 and up to perhaps __10x__ for state-of-the-art networks. 

I encourage you to play with the following widget, especially making GPT-2 compress your texts. 🙂

<CompressionWidget />

The best algorithms in Hutter's competition have compress by a factor of about 9x. But unfortunately, while the challenge is backed by a very deep understanding of the nature of intelligence, the precise parameters of the challenge make it hard to interpret the leading algorithms as the best algorithms in 'understanding'. Since the file size is only 1GB, the compression-by-LLM approach doesn't easily work. You have to append the LLM to the compressed text and it becomes huge. The winning algorithms are thus used on more 'combinatorial' approaches, like those used in ZIP. 

</RiddleExplanation>