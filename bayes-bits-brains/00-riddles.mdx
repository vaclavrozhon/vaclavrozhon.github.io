# A few riddles <a id="introduction"></a>

This site is about probability, information theory, and how it helps us to understand machine learning. 
{/*Our tour guide through this corner of the world is a formula called _Kullback-Leibler (KL) divergence_. */}

Check out [this page](about) for more about the content, assumed background, and logistics. But I hope you get a better feel for what this is by checking out the following riddles. I hope some of them [nerd-snipe](https://xkcd.com/356/) you! 😉 You will understand all of them at the end of this minicourse. 


<RiddleStatement id="intelligence">
Test your intelligence with the following widget! You will be given a bunch of text snippets cut from Wikipedia at a random place. Your job: predict the next letter. You can compare your performance with some neural nets! 

<LetterPredictionWidget/>

Don't feel bad if a machine beats you; they've been studying for this test their entire lives! But why? And why did Claude Shannon - the information theory [GOAT](https://www.merriam-webster.com/dictionary/goat) - make this experiment in the 1940s? 
</RiddleStatement>




<RiddleStatement id="financial-mathematics">

{/* ![Financial Data Distribution](00-riddles/sap_graph.png) */}

Every day, the S&P index price jumps around a bit. <Footnote>If you don't know what the S&P is, just think Bitcoin, Apple stock, or how many euros you can buy with a dollar</Footnote>

I grabbed some historical data, calculated the daily price changes, and normalized them <Footnote>I.e., I am plotting the so-called log-return <Math math = "\ln P_t/P_{t-1} \approx 1 + \frac{P_t - P_{t-1}}{P_{t-1}}" /> for each consecutive daily prices $P_{t-1}, P_t$. </Footnote> and threw them in a histogram. The x-axis shows how big the change was (positive about half the time, negative the other half) and the y-axis shows how often that size change happens. Before I show you the plot, take a guess—what's it gonna look like? 

<ImageGallery images={[{src: "00-introduction/shapes.png", alt: "shapes"}]} fullWidth={true} />

<MultipleChoiceQuestion
  options={["A", "B", "C"]}
  correctIndices={[1, 2]}
  feedbackType="all-show"
  explanation={<>This is hard to say, try it below yourself. I fitted the data by two curves - Gaussian distribution (looks like the shape in C), and Laplace distribution (looks like the shape in B). 
  <FinancialDistributionWidget showBTC={false} showSAP={true} />
  What's happening here? We will discuss this example in the <a href = "05-max_entropy">chapter about the max-entropy principle</a> which is extremely useful whenever we want to model some kind of data. <br/><br/></>}
/>

</RiddleStatement>






<RiddleStatement id="predictions">

It'd be awesome to know the future—or at least know someone who does. So you've gathered some experts and come up with five questions (Q1 to Q5). You ask the experts to give you probabilities for all 5 questions.
A year later, you know what actually happened. Time to find the best expert!  

<ImageGallery images={[{src: "00-introduction/questions.png", alt: "questions"}]} width="75%" />

The question is: who's the best here?

<MultipleChoiceQuestion
  options={["🧑 Expert 1", "👵🏿 Expert 2", "👶 Expert 3"]}
  correctIndices={[0, 1, 2]}
  feedbackType="all-show"
  explanation={<><p>This is actually hard to tell with just 5 questions, we would need many more of them. Expert 🧑 looks pretty impressive, but I am worried about his failed 99%-confidence prediction! Later, we'll see derive the log-score that forecasting tournaments use; this score penalizes failed 99%-confidence predictions quite heavily and we'll get some intuition for why. </p>
  <p>  In the following widget, you can confirm that if Expert 🧑 gave only 80% or 90% probabilities instead of 99%, he would be doing great!</p> 
  <ExpertRatingWidget
    title="Comparing expert predictions"
    showBrierScore={false}
  />  </> }
  />
</RiddleStatement>






<RiddleStatement id="wikipedia">
Marcus Hutter, an AI researcher, started <a href="http://prize.hutter1.net/">this challenge</a>: Take a 1GB file of Wikipedia text. How small can you compress it? <a href="https://en.wikipedia.org/wiki/ZIP_(file_format)">Zipping it</a> gets you about 300MB. But we can do way better. What's the current record? 

<MultipleChoiceQuestion
  options={["around 1MB", "around 10MB", "around 100MB"]}
  correctIndices={[2]}
  explanation={<>It's around 100MB. But wait—why does an <em>AI</em> researcher care about compression? We'll see the connection with <a href="02-crossentropy#coding">entropy</a>, <a href = "02-crossentropy#hutter">next token prediction</a>, and <a href="#">Kolmogorov complexity</a>. In the meantime, you can try to guess how well can various other texts be compressed by various algorithms. 
  <CompressionWidget />
</>}
/> 
</RiddleStatement>

<RiddleStatement id="statistics">

<ImageGallery images={[{src: "00-introduction/rod.png", alt: "rod"}]} fullWidth={true} caption="Determining the length of the foot in [1522 Frankfurt](https://commons.wikimedia.org/wiki/File:Determination_of_the_rute_and_the_feet_in_Frankfurt.png)." />

Back in the day, people measured stuff in feet. Sure, we all kinda know how long a foot is, but eventually you need to nail it down precisely. One way is to grab 16 random people and average their foot lengths:
<Math displayMode={true} math="\hat{\mu} = \frac{1}{16} (X_1 + \dots + X_{16})" />

This gives you a pretty stable number that shouldn't change much if you do it again. To measure how good your estimate is, you'd usually calculate the standard deviation. Remember the formula? Which one's right:

<MultipleChoiceQuestion
  options={[
    <Math math="\hat{\sigma}^2 = \frac{1}{15} \sum_{i=1}^{16} (X_i - \hat{\mu})^2" />,
    <Math math="\hat{\sigma}^2 = \frac{1}{16} \sum_{i=1}^{16} (X_i - \hat{\mu})^2" />,
    <Math math="\hat{\sigma}^2 = \frac{1}{17} \sum_{i=1}^{16} (X_i - \hat{\mu})^2" />
  ]}
  correctIndices={[0, 1, 2]}
  feedbackType="all-show"
  explanation={<>In a sense, they're all correct! Well, more like they're all defensible options from the viewpoint of frequentist statistics. Using <Math math="1/(n-1)" /> gives you the <a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">unbiased estimate</a>, <Math math="1/n" /> gives the <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimate</a>, and <Math math="1/(n+1)" /> minimizes <a href="https://en.wikipedia.org/wiki/Mean_squared_error">the mean squared error</a> between your guess and the truth.<br/><br/>But here's the thing: out of all these ways to estimate stuff, only maximum likelihood became the rockstar of machine learning. Using KL divergence, <a href="04-minimizing#mle">we'll see</a> why maximum likelihood is so special and what makes it tick.</>}
/>
</RiddleStatement>


{/*
<Expand headline="🧠 LLM training"> <a id="deep-learning"></a>

So you wanna train a large language model (LLM) like GPT/Gemini/Claude. These beasts take text, do some crazy computations, and spit out the next letter (Well, actually the next [token](https://en.wikipedia.org/wiki/Large_language_model#Tokenization), which is like a little chunk of letters).
LLMs don't just guess a single letter—they predict the whole distribution $p$ of what might come next. 

Training LLMs is complicated as hell, but one super important bit is picking the _loss function_. Here's the deal: take some text that shows up everywhere online, like "My name is A". We know what usually comes next—maybe "l" is common (all those Alexes), but "a" not so much. Call this the ground-truth distribution $p$.

Your LLM tries to guess this distribution with its own guess $q$. We need to measure how close $q$ is to $p$ using a loss function, then make that number as small as possible during training. So given distributions $p = \{p_1, \dots, p_k\}$ and $q = \{q_1, \dots, q_k\}$, which loss function should we pick?

<MultipleChoiceQuestion
  options={[
    <><Math math="\mathcal L(p,q) = \sum_{i = 1}^k |p_i - q_i|" /></>,
    <><Math math="\mathcal L(p,q) = \sum_{i = 1}^k (p_i - q_i)^2" /></>,
    <><Math math="\mathcal L(p,q) = \sum_{i = 1}^k p_i \cdot \log (p_i / q_i)" /></>
  ]}
  correctIndices={[2]}
  explanation={<>The first two work okay, but practioners typically use KL divergence. Here's why it's nice: if <Math math="p_i = 0.5" /> and <Math math="q_i = 0.49" />, KL thinks that's basically fine. But if <Math math="p_i = 0.01" /> and <Math math="q_i = 0.0" />, KL freaks out (the ratio <Math math="p_i/q_i" /> is infinite). On the other hand, the other two loss functions think both situations are similar problem. <br/><br/>Check out this widget with two distributions and all three loss functions. <br/><br/><DistributionComparisonWidget title="KL Divergence Explorer" /></>}
/>
</Expand>
*/}


<RiddleStatement id="xkcd">

<a href="https://www.explainxkcd.com/wiki/index.php/1159:_Countdown" target="_blank" rel="noopener noreferrer">
  <ImageGallery images={[{src: "00-introduction/countdown.png", alt: "Are the odds in our favor?"}]} fullWidth={true} />
</a>

So... are the odds in our favor?

<MultipleChoiceQuestion
  options={["Yes", "No", "depends"]}
  correctIndices={[1, 2]}
  explanation={<>
  <p>The guy in the hat seems to be confident that at least one digit behind the picture is not zero. If those digits are uniformly random, it's a safe assumption. However, we will use the maximum entropy principle to justify a different probabilistic model and confirm our gut feeling: Given all the zeros we can see, it's actually very plausible that all other hidden digits are zeros too.</p>

  <p>More on that <a href="05-max_entropy#xkcd">later</a>; We will use the widget below to confirm that there's a huge difference between seing "00002382" and just "2382".</p>

  <XKCDCountdownWidget />
  </>}
/> 
</RiddleStatement>

{/*
<RiddleStatement id="machine-learning">

When you first dive into machine learning, it looks like total chaos. There's linear regression, and logistic regression, and $k$-means. All of those  look like a bunch of random tricks and optimization problems. 

Then, there are neural networks. Very impressive, but still pretty daunting. Take the standard architecture for image generation called a variational autoencoder. You train it by optimizing this absolute monster of a function: 

<Math displayMode={true} math = "\frac{1}{N} \sum_{i = 1}^N \left( \sum_y p'(y | x) \frac{\| X_i - \textrm{Dec}(y)\|^2}{2d} \,+\, \left( \frac12 \sum_{j = 1}^d \textrm{Enc}_{\mu, j}(X_i)^2 + \textrm{Enc}_{\sigma^2, j}(X_i) - \log \textrm{Enc}_{\sigma^2, j}(X_i) \right)\right)"/>

Where the hell are all these, so-called "loss functions", coming from? [We'll see](07-machine_learning) how KL divergence & friends make sense of this mess—and tons of other standard ML algorithms too, like those in the widget below. 

<MLProblemExplorerSimple defaultMode="logisticRegression" />

</RiddleStatement>
*/}




## What's next? <a id="next-steps"></a>

As we go through the mini-course, we'll revisit each puzzle and understand what's going on. But this text is not just about the riddles. It's about explaining some mathematics that I find cool and important, with focus on getting solid theoretical background behind current machine learning. 

There are three parts to this website. 

1. First (first three chapters), we will start by recalling the Bayes' rule. Playing with probabilities will help us understand KL divergence, cross-entropy and entropy. Those are the secret weapons that solve all our riddles. 
2. Then (next three chapters), we will see that minimizing KL divergence is a powerful principle to build good probabilistic models. We will use it to explain most of our riddles, but even better, we'll get pretty good understanding of a super-important part of machine-learning -- setting up loss functions.
3. Finally (final two chapters), we will dig into how entropy is related to coding theory and compression. This can give us pretty useful intuitions about large language models. 

This is your last chance. You can go on with your life and believe whatever you want to believe about KL divergence. Or you go to [the first chapter](01-kl_intro) and see how far the rabbit-hole goes. 


<DualActionImage src="fig/pills.png" alt="pills" width="75%" />
